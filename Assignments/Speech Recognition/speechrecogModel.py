# -*- coding: utf-8 -*-
"""SpeechRecogModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G9sAY_PbBsgIG_rkRpeZA_BSYFkE-xNm
"""

import tensorflow as tf

GAMMA=0.997
EPSILON=1e-5
FILTERS=32
RNNS_TYPE = {
    "lstm": tf.contrib.rnn.BasicLSTMCell,
    "rnn": tf.contrib.rnn.RNNCell,
    "gru": tf.contrib.rnn.GRUCell,
}
def rnn_layer(X,rnn_cell,rnn_hidden_size,layer_id,batch_norm,is_Bidirectional,training):
  if(batch_norm):
    X=tf.keras.layers.BatchNormalization(inputs=X,momentum=GAMMA,epsilon=EPSILON,fused=True,training=training)
  fw_cell = rnn_cell(num_units=rnn_hidden_size,
                     name="rnn_fw_{}".format(layer_id))
  bw_cell = rnn_cell(num_units=rnn_hidden_size,
                     name="rnn_bw_{}".format(layer_id))
  if(is_Bidirectional):
    outputs,_=tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_cell,cell_bw=bw_cell,inputs=X,dtype=tf.float32,swap_memory=True)
    rnn_outputs=tf.concat(outputs,-1)
  else:
    rnn_outputs=tf.nn.dynamic_rnn(fw_cell,X,dtype=tf.float32,swap_memory=True)
  return rnn_outputs

class DeepSpeechModel(object):
  def __init__(self,rnn_layers_count,rnn_type,is_Bidirectional,rnn_hidden_size,num_classes,bias):
    self.rnn_layers_count = rnn_layers_count
    self.rnn_type = rnn_type
    self.is_Bidirectional = is_Bidirectional
    self.rnn_hidden_size = rnn_hidden_size
    self.num_classes = num_classes
    self.bias=bias
  def __call__(self,X,training):
    X=tf.pad(X,[[0, 0], [padding[0], padding[0]], [padding[1], padding[1]] [0, 0]])
    X=tf.keras.layers.Conv2D(X,padding=(20,5),filters=FILTERS,kernel_size=(41,11),strides=(2,2),bias=False,activation=tf.nn.relu6,layer_id=1,training=training)
    X=tf.keras.layers.BatchNormalization(inputs=X,momentum=GAMMA,epsilon=EPSILON,fused=True,training=training)
    X=tf.pad(X,[[0, 0], [padding[0], padding[0]], [padding[1], padding[1]] [0, 0]])
    X=tf.keras.layers.Conv2D(X,padding=(10,5),filters=FILTERS,kernel_size=(21,11),strides=(2,1),layer_id=2,training=training)
    X=tf.keras.layers.BatchNormalization(inputs=X,momentum=GAMMA,epsilon=EPSILON,fused=True,training=training)
    batch_size=tf.size(X)[0]
    feature_size=X.get_shape().as_list()[2]
    X=tf.reshape(X,[batch_size,-1,feature_size*FILTERS])
    rnn_cell=RNN_TYPE[self.rnn_type]
    for i in range(self.rnn_layers_count):
      batch_norm=(i!=0)
      X=rnn_layer(X,rnn_cell,self.rnn_hidden_size,i+1,batch_norm,is_Bidirectional,training)
    X=tf.keras.layers.BatchNormalization(inputs=X,momentum=GAMMA,epsilon=EPSILON,fused=True,training=training)
    logits=tf.keras.layers.Dense(X,self.num_classes,bias=self.bias)
    return logits

